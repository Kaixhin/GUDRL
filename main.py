from datetime import datetime
import os

import numpy as np
import torch
from tqdm import tqdm

from agent import Agent
from args import get_argparser
from environment import Env
from memory import Memory
from plotting import lineplot, scatterplot
from testing import calibration, test


# Experiment setup
args = get_argparser().parse_args()
np.random.seed(args.seed)
torch.manual_seed(args.seed)
os.makedirs('results', exist_ok=True)
exp_id = datetime.utcnow().replace(microsecond=0).isoformat()  # ID experiment by current time


# Create environment, agent, memory and metrics
env = Env(args.mode)
agent = Agent(env.observation_size, env.action_size, env.goal_size, args.hidden_size, args.learning_rate, args.weight_decay)
memory = Memory(args.mem_size, env.observation_size, env.action_size, env.goal_size)
metrics = {'train_returns': [], 'test_returns': [], 'losses': []}


# Offline training
if args.mode in ['imitation', 'offline']:
  memory.load('memory.pth', args.mode)
  for _ in tqdm(range(1, args.offline_iter + 1)):
    metrics['losses'].append(agent.train(memory, args.batch_size, args.seq_len, args.mode))

  agent.set_mode(training=False)
  metrics['test_returns'].append(test(agent, memory, args.mode, args.test_episodes))

  lineplot(np.arange(100), 100 * metrics['test_returns'], f'{exp_id}_test_returns', '', 'Return', ylim=(0, 500), baseline_y=memory.get_avg_return())  # Plot a straight line
  lineplot(np.arange(args.offline_iter), metrics['losses'], f'{exp_id}_losses', 'Iteration', 'Loss')

# Online training
else:
  terminal = True
  for step in tqdm(range(1, args.steps + 1)):
    if terminal:
      (observation, goal), terminal, total_reward = env.reset(), False, 0
      command, hidden = agent.get_initial_command(goal, memory, args.mode), None  # Initial hidden state generated by agent when given None
    
    if args.render: env.render()
    with torch.inference_mode(): policy, hidden = agent.observe(observation, command, hidden)
    action = policy.sample()
    next_observation, goal, reward, terminal = env.step(action)
    memory.update(observation, goal, action, reward, terminal)  # Update memory
    agent.update_command(observation, goal, action, reward, terminal, command, args.mode)  # Update command (inplace)
    total_reward += reward
    observation = next_observation

    if step >= args.train_start:
      if step % args.train_interval == 0:
        metrics['losses'].append(agent.train(memory, args.batch_size, args.seq_len, args.mode))

      if step % args.test_interval == 0:
        agent.set_mode(training=False)
        metrics['test_returns'].append(test(agent, memory, args.mode, args.test_episodes))
        agent.set_mode(training=True)

      if step % args.log_interval == 0:
        lineplot(np.arange(len(metrics['train_returns'])), metrics['train_returns'], f'{exp_id}_train_returns', 'Episode', 'Return', ylim=(0, 500 * (env.eps_per_trial if args.mode == 'meta' else 1)))
        lineplot(np.arange(args.train_start, step + 1, args.test_interval) / 1000, metrics['test_returns'], f'{exp_id}_test_returns', 'Step (x 1000)', 'Return', ylim=(0, 500 * (env.eps_per_trial if args.mode == 'meta' else 1)))
        lineplot(np.arange(args.train_start, step + 1, args.train_interval) / 1000, metrics['losses'], f'{exp_id}_losses', 'Step (x 1000)', 'Loss')
    
    if terminal: metrics['train_returns'].append(total_reward)

  if args.save_mem: memory.save(f'{exp_id}_memory')

# Calibration plots
if args.mode in ['online', 'offline']:
  agent.set_mode(training=False)
  achieved_returns, desired_returns = calibration(agent, memory, args.mode, args.test_episodes)
  scatterplot(np.array(desired_returns), np.array(achieved_returns), f'{exp_id}_calibration', 'Desired Returns', 'Achieved Returns', xlim=(0, 500), ylim=(0, 500), calibration_line=True)

torch.save(metrics, f'results/{exp_id}_metrics.pth')
